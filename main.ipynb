{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Set up Dependencies and Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install python3-pip\n",
    "!pip3 install jupyter tensorflow==2.19.0 segmentation-models-3D h5py numpy matplotlib scikit-learn patchify kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure kaggle.json is in ~/.kaggle/ with chmod 600\n",
    "!kaggle datasets download -d awsaf49/brats20-dataset-training-validation -p /root/BraTS2020\n",
    "!unzip /root/BraTS2020/brats20-dataset-training-validation.zip -d /root/BraTS2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports and GPU Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import segmentation_models_3D as sm\n",
    "import numpy as np\n",
    "import h5py\n",
    "from patchify import patchify\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import sys\n",
    "from IPython.utils import io\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set up logging\n",
    "log_file = os.path.join(output_dir, \"training_log.txt\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler(log_file), logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "class LoggerWriter:\n",
    "    def __init__(self, logger, level):\n",
    "        self.logger = logger\n",
    "        self.level = level\n",
    "    def write(self, message):\n",
    "        if message.strip():\n",
    "            self.logger.log(self.level, message)\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "sys.stdout = LoggerWriter(logging.getLogger(), logging.INFO)\n",
    "\n",
    "# GPU setup with mixed precision\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logging.info(f\"Found GPU: {gpus[0]}\")\n",
    "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "        logging.info(f\"TensorFlow version: {tf.__version__}\")\n",
    "    except RuntimeError as e:\n",
    "        logging.error(f\"GPU setup error: {e}\")\n",
    "else:\n",
    "    logging.error(f\"No GPU detected. Available devices: {tf.config.list_physical_devices()}\")\n",
    "    raise SystemError('GPU device not found')\n",
    "\n",
    "logging.info(\"Cell 1 completed: Setup and GPU configuration done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(file_path, modality='t1ce'):\n",
    "    \"\"\"Loads a modality from an HDF5 file.\"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        img = f[modality][:]\n",
    "        return img.astype(np.float32)\n",
    "\n",
    "def visualize_mri_and_segmentation(mri_data, seg_data, slice_idx=None):\n",
    "    \"\"\"Visualize MRI scan and segmentation mask.\"\"\"\n",
    "    if slice_idx is None:\n",
    "        slice_idx = mri_data.shape[2] // 2\n",
    "    logging.info(f\"MRI Shape: {mri_data.shape}, Segmentation Shape: {seg_data.shape}\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(mri_data[:, :, slice_idx], cmap=\"gray\")\n",
    "    axes[0].set_title(f\"MRI Scan (T1CE) - Slice {slice_idx}\")\n",
    "    axes[1].imshow(seg_data[:, :, slice_idx], cmap=\"gray\")\n",
    "    axes[1].set_title(f\"Segmentation Mask - Slice {slice_idx}\")\n",
    "    plt.savefig(os.path.join(output_dir, f\"mri_seg_slice_{slice_idx}.png\"))\n",
    "    plt.close()\n",
    "    logging.info(f\"Saved visualization to {output_dir}/mri_seg_slice_{slice_idx}.png\")\n",
    "\n",
    "def load_data_from_folder(folder_path, max_subfolders=369, chunk_size=20):\n",
    "    \"\"\"Load MRI and segmentation data from .h5 files in chunks.\"\"\"\n",
    "    mri_data_dict = {}\n",
    "    seg_data_dict = {}\n",
    "    subfolders = sorted([f for f in os.listdir(folder_path) if 'BraTS20_Training_' in f])[:max_subfolders]\n",
    "    \n",
    "    for chunk_start in range(0, len(subfolders), chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, len(subfolders))\n",
    "        logging.info(f\"Loading chunk {chunk_start}-{chunk_end}\")\n",
    "        for subfolder in subfolders[chunk_start:chunk_end]:\n",
    "            subfolder_path = os.path.join(folder_path, subfolder)\n",
    "            h5_file = os.path.join(subfolder_path, f\"{subfolder}.h5\")\n",
    "            if os.path.isfile(h5_file):\n",
    "                logging.info(f\"Loading data from {subfolder}\")\n",
    "                mri_data_dict[subfolder] = {\n",
    "                    't1': load_h5(h5_file, 't1'),\n",
    "                    't1ce': load_h5(h5_file, 't1ce'),\n",
    "                    't2': load_h5(h5_file, 't2'),\n",
    "                    'flair': load_h5(h5_file, 'flair')\n",
    "                }\n",
    "                seg_data_dict[subfolder] = load_h5(h5_file, 'seg')\n",
    "    \n",
    "    first_key = list(mri_data_dict.keys())[0]\n",
    "    visualize_mri_and_segmentation(mri_data_dict[first_key]['t1ce'], seg_data_dict[first_key])\n",
    "    logging.info(\"Cell 2 completed: Data loaded.\")\n",
    "    return mri_data_dict, seg_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Patch Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(mri_data_dict, seg_data_dict, patch_size=(64, 64, 64, 4), step=32):\n",
    "    \"\"\"Extract patches from stacked MRI modalities and segmentation data.\"\"\"\n",
    "    mri_patches_dict = {}\n",
    "    seg_patches_dict = {}\n",
    "    \n",
    "    for subfolder in mri_data_dict.keys():\n",
    "        logging.info(f\"Processing subfolder: {subfolder}\")\n",
    "        modalities = mri_data_dict[subfolder]\n",
    "        \n",
    "        mri_data_stacked = np.stack([modalities['t1'], modalities['t1ce'], \n",
    "                                    modalities['t2'], modalities['flair']], axis=-1)\n",
    "        target_shape = tuple(((dim + step - 1) // step) * step for dim in mri_data_stacked.shape[:-1]) + (4,)\n",
    "        pad_width = [(0, target_shape[i] - mri_data_stacked.shape[i]) for i in range(3)] + [(0, 0)]\n",
    "        mri_data_padded = np.pad(mri_data_stacked, pad_width, mode='constant', constant_values=0)\n",
    "        \n",
    "        mri_patches = patchify(mri_data_padded, patch_size=patch_size, step=step)\n",
    "        logging.info(f\"MRI patches shape: {mri_patches.shape}\")\n",
    "        \n",
    "        seg_data = seg_data_dict[subfolder]\n",
    "        seg_target_shape = tuple(((dim + step - 1) // step) * step for dim in seg_data.shape)\n",
    "        seg_pad_width = [(0, seg_target_shape[i] - seg_data.shape[i]) for i in range(3)]\n",
    "        seg_data_padded = np.pad(seg_data, seg_pad_width, mode='constant', constant_values=0)\n",
    "        seg_patches = patchify(seg_data_padded, patch_size=patch_size[:-1], step=step)\n",
    "        logging.info(f\"Segmentation patches shape: {seg_patches.shape}\")\n",
    "        \n",
    "        mri_patches_dict[subfolder] = mri_patches\n",
    "        seg_patches_dict[subfolder] = seg_patches\n",
    "    \n",
    "    logging.info(\"Cell 3 completed: Patches extracted.\")\n",
    "    return mri_patches_dict, seg_patches_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(mri_patches_dict, seg_patches_dict, chunk_size=20):\n",
    "    \"\"\"Normalize and preprocess patches in chunks.\"\"\"\n",
    "    mri_patches = []\n",
    "    seg_patches = []\n",
    "    subfolders = list(mri_patches_dict.keys())\n",
    "    \n",
    "    for chunk_start in range(0, len(subfolders), chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, len(subfolders))\n",
    "        logging.info(f\"Preprocessing chunk {chunk_start}-{chunk_end}\")\n",
    "        chunk_mri = []\n",
    "        chunk_seg = []\n",
    "        \n",
    "        for subfolder in subfolders[chunk_start:chunk_end]:\n",
    "            mri_patch = mri_patches_dict[subfolder]\n",
    "            seg_patch = seg_patches_dict[subfolder]\n",
    "            \n",
    "            mri_patch = (mri_patch - np.mean(mri_patch)) / (np.std(mri_patch) + 1e-8)\n",
    "            num_patches = np.prod(mri_patch.shape[:-4])\n",
    "            mri_patch = mri_patch.reshape(num_patches, *mri_patch.shape[-4:])\n",
    "            seg_patch = seg_patch.reshape(num_patches, *seg_patch.shape[-3:])\n",
    "            seg_patch = to_categorical(seg_patch, num_classes=4)\n",
    "            \n",
    "            chunk_mri.append(mri_patch)\n",
    "            chunk_seg.append(seg_patch)\n",
    "        \n",
    "        mri_patches.append(np.concatenate(chunk_mri, axis=0))\n",
    "        seg_patches.append(np.concatenate(chunk_seg, axis=0))\n",
    "        del chunk_mri, chunk_seg\n",
    "        K.clear_session()  # Clear GPU memory\n",
    "    \n",
    "    X = np.concatenate(mri_patches, axis=0)\n",
    "    y = np.concatenate(seg_patches, axis=0)\n",
    "    logging.info(f\"Preprocessed data shapes - X: {X.shape}, y: {y.shape}\")\n",
    "    logging.info(\"Cell 4 completed: Data preprocessed.\")\n",
    "    return X, y\n",
    "\n",
    "# Load and process data\n",
    "with io.capture_output() as captured:\n",
    "    folder_path = \"/root/BraTS2020/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
    "    mri_data_dict, seg_data_dict = load_data_from_folder(folder_path)\n",
    "    mri_patches_dict, seg_patches_dict = extract_patches(mri_data_dict, seg_patches_dict)\n",
    "    X, y = preprocess_data(mri_patches_dict, seg_patches_dict)\n",
    "logging.info(f\"Cell 4 output:\\n{captured.stdout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Split and Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "with io.capture_output() as captured:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training data shape: {X_train.shape}, Validation data shape: {X_val.shape}\")\n",
    "logging.info(f\"Cell 5 output (split):\\n{captured.stdout}\")\n",
    "\n",
    "# Define model\n",
    "model = sm.Unet(\n",
    "    backbone_name='resnet50',\n",
    "    input_shape=(64, 64, 64, 4),\n",
    "    classes=4,\n",
    "    activation='softmax',\n",
    "    encoder_weights=None\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=sm.losses.DiceLoss(),\n",
    "    metrics=[sm.metrics.IOUScore()]\n",
    ")\n",
    "\n",
    "with io.capture_output() as captured:\n",
    "    model.summary()\n",
    "logging.info(f\"Model summary:\\n{captured.stdout}\")\n",
    "logging.info(\"Cell 5 completed: Data split and model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training and Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "batch_size = 4\n",
    "epochs = 30\n",
    "with io.capture_output() as captured:\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1\n",
    "    )\n",
    "logging.info(f\"Training output:\\n{captured.stdout}\")\n",
    "\n",
    "# Plot and save training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, \"loss_plot.png\"))\n",
    "plt.close()\n",
    "logging.info(f\"Saved loss plot to {output_dir}/loss_plot.png\")\n",
    "\n",
    "# Save model\n",
    "model_file = os.path.join(output_dir, \"3d_unet_brats2020.h5\")\n",
    "model.save(model_file)\n",
    "logging.info(f\"Model saved to {model_file}\")\n",
    "logging.info(\"Cell 6 completed: Training completed and outputs saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
